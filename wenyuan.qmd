---
title: "Alcohol Data Tidying Report"
format: html
editor: visual
---

# 1 Introduction

In this report, we are going to present the process and rationale of our data tidying process of the data set `AlcoholData`, in terms of how the four data tidy principles were applied, how the relations and primary keys of each tibble were found and checked, and the way we joined the tidied tibbles. All data tidying and report writing were done in R studio version 2025.09.2+418.

# 2 Data Tidying

## 2.1 Data Importing

All eight data sets were imported with `read_csv` under `tidyverse` and made use of.

```{r}
#| label: data-importing

# tidyverse for data wrangling
library(tidyverse)

# Import and rename data sets for clarity
comment <- read_csv("AlcoholData/Comment.csv")
day_data <- read_csv("AlcoholData/DayData.csv")
like <- read_csv("AlcoholData/Like.csv")
login <- read_csv("AlcoholData/Login.csv")
post <- read_csv("AlcoholData/Post.csv")
pre_survey <- read_csv("AlcoholData/Presurvey.csv")
user <- read_csv("AlcoholData/User.csv")
week_data <- read_csv("AlcoholData/WeekData.csv")
```

## 2.2 Principle 1: Each type of case must have its own tibble

To decide if principle 1 was violated in each data set, we used `group_by()`, `summarise()` and `count()` to identify type of cases in each raw data set.

### 2.2.1 Comment.csv

The type of case in this data set is an individual comment made by a user on a post at a certain time, defined by (`post_id`, `commenter_id`, `comment_time`). We assessed whether there were duplicate rows for the same comment event, and also whether attributes such as `comment_content` vary within the same case (i.e., for the same `post_id`, `commenter_id`, `comment_time`).

Principle 1 was not violated in this set as all variables belong to the same type of case.

```{r}
# Type of case check
comment |> 
  group_by(post_id, comment_time, commenter_id) |> 
  summarise(n_distinct = n_distinct(comment_content), .groups = "drop") |> 
  count(n_distinct)

# Check for duplicates
comment_dupes <- comment |>
  count(post_id, commenter_id, comment_time) |>
  filter(n > 1)

# View duplicate comments
comment |> semi_join(comment_dupes, by = c("post_id", "commenter_id", "comment_time"))
```

To ensure that each comment event appears only once, we removed duplicates and retain all available information for the first occurrence using `.keep_all = TRUE` in `distinct()`. This preserves additional columns associated with each comment, which may be required elsewhere in analysis.

```{r}
# Use .keep_all = TRUE so that, for each group of duplicated identifiers, we retain all columns from the first occurrence and preserve other information in the tibble.
comment_tidied <- comment |>
  distinct(post_id, commenter_id, comment_time, .keep_all = TRUE)
```

Each row is now an individual comment action (by a commenter, on a post, at a time).

### 2.2.2 DayData.csv

### 2.2.3 Like.csv

The type of case in `Like.csv` is a unique like event, defined by (`post_id`, `liker_id`). It does not violate principle 1 as well, we checked for duplicate (`post_id`, `liker_id`) pairs to ensure data set clarity.

```{r}
# Check for duplicates
like_dupes <- like |>
  count(post_id, liker_id) |>
  filter(n > 1)

# View duplicate likers
like |> semi_join(like_dupes, by = c("post_id", "liker_id"))
```

We removed any duplicate rows so each row corresponds to a unique like event.

```{r}
# .keep_all ensures other information is preserved.
like_tidied <- like |>
  distinct(post_id, liker_id, .keep_all = TRUE)
```

Each row now is a unique user liking a post.

### 2.2.4 Login.csv

In this data set, the case type is a login event, defined by (`user_id`, `user_login_time`). Principle 1 was not violated here, but we checked for duplicate login events per user to prevent further analytic errors.

```{r}
# Check for duplicates
login_dupes <- login |>
  count(user_id, user_login_time) |>
  filter(n > 1)
```

No duplicated rows were found. Each row is a single login event, representing a login by a user at a specific time.

### 2.2.5 Post.csv

We checked for the possible type of cases in `Post.csv`.`viewer_id` was found not belong to the type of case of "post", which means principle 1 was violated.

```{r}
# Check for violation (multiple views per post)
post |> 
  group_by(post_id) |>
  summarise(n_distinct = n_distinct(viewer_id), .groups = "drop") |>
  count(n_distinct)
```

The raw data set `Post.csv` was then split into two separate tibbles based on the type of cases. `post_sub1` included information about each post, `post_sub2` contained only two variables: `post_id` and `viwer_id`. `distinct()` was used on the primary key(s) to remove duplicates. `post_sub1` was thus reduced to a parsimonious data set of 547 rows from 91031 rows.

```{r}
# Splitting post and view information into 2 separate tibbles
# Adjust to select post-only variables
# Each row: one post
post_sub1 <- post |> 
  select(post_id : sync_time) |> 
  distinct(post_id, .keep_all = TRUE) 

# View events
# Each row: one unique (post, viewer) event
post_sub2 <- post |> 
  select(post_id, viewer_id) |> 
  distinct(post_id, viewer_id, .keep_all = TRUE)
```

Now for `post_sub1`, each row is a unique post; for `post_sub2`, each row is a unique view event (a user viewing a post).

### 2.2.6 Presurvey.csv

This file...

### 2.2.7 User.csv

### 2.2.8 WeekData.csv

## 2.3 Principle 2: Each observation must have its own row

## 2.4 Principle 3: Each variable must have its own column

## 2.5 Principle 4: Each value must have its own cell

# 3 Table relations and primary key checking

## 3.1 Comment.csv

```{r}
comment_tidied |> 
  count(post_id, comment_time, commenter_id) |> 
  filter(n > 1)
```

## 3.3 Like.csv

```{r}
like_tidied |> 
  count(post_id, liker_id) |> 
  filter(n > 1)
```

## 3.4 Login.csv

```{r}
login |> 
  count(user_id, user_login_time) |> 
  filter(n > 1)
```

## 3.5 Post.csv

### 3.5.1 post_sub1

```{r}
post_sub1 |> 
  count(post_id) |> 
  filter(n > 1)
```

### 3.5.2 post_sub2

```{r}
post_sub2 |> 
  count(post_id, viewer_id) |> 
  filter(n > 1)
```

# 4 Table Joining

# 5 Description
