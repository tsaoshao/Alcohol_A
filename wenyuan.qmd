---
title: "Alcohol Data Tidying Report"
format: html
editor: visual
---

# 1 Introduction

In this report, we are going to present the process and rationale of our data tidying process of the data set `AlcoholData`, in terms of how the four data tidy principles were applied, how the relations and primary keys of each tibble were found and checked, and the way we joined the tidied tibbles. All data tidying and report writing were done in R studio version 2025.09.2+418.

# 2 Data Tidying

## 2.1 Data Importing

All eight data sets were imported with `read_csv` under `tidyverse` and made use of.

```{r results='hide'}
#| label: data-importing

# tidyverse for data wrangling
library(tidyverse)

# Import and rename data sets for clarity
comment <- read_csv("AlcoholData/Comment.csv")
day_data <- read_csv("AlcoholData/DayData.csv")
like <- read_csv("AlcoholData/Like.csv")
login <- read_csv("AlcoholData/Login.csv")
post <- read_csv("AlcoholData/Post.csv")
pre_survey <- read_csv("AlcoholData/Presurvey.csv")
user <- read_csv("AlcoholData/User.csv")
week_data <- read_csv("AlcoholData/WeekData.csv")
```

## 2.2 Principle 1: Each type of case must have its own tibble

To decide if principle 1 was violated in each data set, we used `group_by()`, `summarise()` and `count()` to identify type of cases in each raw data set.

### 2.2.1 Comment.csv

The type of case in this data set is an individual comment made by a user on a post at a certain time, defined by a composite primary key (`PostID`, `CommenterID`, `CommentTime`). We assessed whether there were duplicate rows for the same comment event, and also whether attributes such as `CommentContent` vary within the same case (i.e., for the same `PostID`, `CommenterID`, `CommentTime`).

Principle 1 was not violated in this data set as all variables belong to the same type of case.

```{r}
# Type of case check
comment |> 
  group_by(PostID, CommentTime, CommenterID) |> 
  summarise(n_distinct = n_distinct(CommentContent), .groups = "drop") |> 
  count(n_distinct)

# Check for primary key and duplicates
comment_dupes <- comment |>
  count(PostID, CommenterID, CommentTime) |>
  filter(n > 1)

# View duplicate comments
comment |> semi_join(comment_dupes, by = c("PostID", "CommenterID", "CommentTime"))
```

After checking for duplicates, we checked for missing values in the primary key columns. Rows with missing keys cannot be reliably identified or linked across tables, and should be addressed during data cleaning.

```{r}
# Check for missing values in primary key columns
comment |> 
  filter(is.na(PostID) | is.na(CommenterID) | is.na(CommentTime))
```

As no missing value showed up, we proceeded with the cleaning process.

To ensure that each comment event appears only once, we removed duplicates and retain all available information for the first occurrence using `.keep_all = TRUE` in `distinct()`. This preserves additional columns associated with each comment, which may be required elsewhere in analysis.

```{r}
# retain all columns from the first occurrence and preserve other information in the tibble.
comment_tidy <- comment |>
  distinct(PostID, CommenterID, CommentTime, .keep_all = TRUE)
```

Each row is now an individual comment action (by a commenter, on a post, at a time).

### 2.2.2 DayData.csv

### 2.2.3 Like.csv

The type of case in `Like.csv` is a unique like event, defined by (`PostID`, `LikerID`). It does not violate principle 1 as well, we checked for duplicate and missing values in (`PostID`, `LikerID`) pairs to ensure data set clarity.

```{r}
# Check for duplicates
like_dupes <- like |>
  count(PostID, LikerID) |>
  filter(n > 1)

# View duplicate likers
like |> semi_join(like_dupes, by = c("PostID", "LikerID"))

# Check for missing values in primary key columns
like |> 
  filter(is.na(PostID) | is.na(LikerID))
```

We removed any duplicate rows so each row corresponds to a unique like event.

```{r}
# .keep_all ensures other information is preserved.
like_tidy <- like |>
  distinct(PostID, LikerID, .keep_all = TRUE)
```

Each row now is a unique user liking a post.

### 2.2.4 Login.csv

In this data set, the case type is a login event, defined by `UserID` and `User_LoginTime`. Principle 1 was not violated here, but we checked for duplicate login events per user and missing values to prevent  analytic errors.

```{r}
# Check for duplicates
login_dupes <- login |>
  count(UserID, User_LoginTime) |>
  filter(n > 1)

# Check for missing values in primary key columns
login |> 
  filter(is.na(UserID) | is.na(User_LoginTime))
```

No duplicated rows or missing values were found. Each row is a single login event, representing a login by a user at a specific time.

### 2.2.5 Post.csv

We checked for the possible type of cases in `Post.csv`.`ViewerID` was found not belong to the type of case of "post", which means principle 1 was violated. In the missing value check, we found a missing value in the `ViewerID` column. To keep most meaningful information in that observation, we only removed the missing value when splitting a new tibble for `PostID` /* `ViewerID`.

```{r}
# Check for violation (multiple views per post)
post |> 
  group_by(PostID) |>
  summarise(n_distinct = n_distinct(ViewerID), .groups = "drop") |>
  count(n_distinct)

# Check for missing values in primary key columns
post |> 
  filter(is.na(PostID) | is.na(ViewerID))
```

The raw data set `Post.csv` was then split into two separate tibbles based on the type of cases. `post_sub1` included information about each post, `post_sub2` depicted how each post was viewed by a unique viewer, containing only two variables: `PostID` and `ViewerID`. `distinct()` was used on the primary key(s) to remove duplicates. `post_sub1` was thus reduced to a parsimonious data set of 547 rows from 91031 rows.

```{r}
# Splitting post and view information into 2 separate tibbles
# Adjust to select post-only variables
# Each row: one post
post_sub1 <- post |> 
  select(PostID : SyncTime) |> 
  distinct(PostID, .keep_all = TRUE) 

# View events
# Each row: one unique (post, viewer) event
post_sub2 <- post |> 
  select(PostID, ViewerID) |> 
  filter(!is.na(ViewerID)) |>
  distinct(PostID, ViewerID, .keep_all = TRUE)
```

Now for `post_sub1`, each row is a unique post and its attributes; for `post_sub2`, each row is a unique view event (a user viewing a post).

### 2.2.6 Presurvey.csv

This file...

### 2.2.7 User.csv

### 2.2.8 WeekData.csv

## 2.3 Principle 2: Each observation must have its own row

## 2.4 Principle 3: Each variable must have its own column

## 2.5 Principle 4: Each value must have its own cell

# 3 Table relations and primary key checking

## 3.1 Comment.csv

```{r}
comment_tidy |> 
  count(PostID, CommentTime, CommenterID) |> 
  filter(n > 1)
```

## 3.3 Like.csv

```{r}
like_tidy |> 
  count(PostID, LikerID) |> 
  filter(n > 1)
```

## 3.4 Login.csv

```{r}
login |> 
  count(UserID, User_LoginTime) |> 
  filter(n > 1)
```

## 3.5 Post.csv

### 3.5.1 post_sub1

```{r}
post_sub1 |> 
  count(PostID) |> 
  filter(n > 1)
```

### 3.5.2 post_sub2

```{r}
post_sub2 |> 
  count(PostID, ViewerID) |> 
  filter(n > 1)
```

# 4 Table Joining

# 5 Description
