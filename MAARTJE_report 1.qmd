---
title: "report"
format: html
editor: visual
---

# Alcohol Data Tidying Report

## Introduction

... This is a student project of data tidying in R studio.

```{r results = 'hide'}
#| label: data-importing

library(tidyverse)


DayData <- read_csv("/Users/maartje/Desktop/Universiteit van Amsterdam/UvA Research Master year 2/R/Data project GROUP/Alcohol_A/AlcoholDataSet/DayData.csv")
WeekData <- read_csv("/Users/maartje/Desktop/Universiteit van Amsterdam/UvA Research Master year 2/R/Data project GROUP/Alcohol_A/AlcoholDataSet/WeekData.csv")
```

## Table relations and primary key checking

### DayData.csv

For checking the primary key, we first looked at how many people were in the `Day Dataset`. At first glance, the primary key was `UserID` x `day_nr`. We found that there are multiple rows per person per day, with at least 207 instances where this occurs.

```{r}

DayData_tidy <- DayData

#check how many rows in the daydataset.
nrow(DayData_tidy)
#so now we know there are 10019 rows in the daydataset. 

#check how many persons are included in the daydataset.
DayData_tidy %>% 
  count(UserID)
#based on this we can say there are 282 persons in the daydataset (nrows = 282)

#finding out primary key
DayData_tidy|> 
  count(UserID, day_nr,) %>% 
  filter(n > 1) 
#here we found that there are multiple rows per person per day. 

#check how many multiple rows per person per day are in the dataset.
DayData_tidy|> 
  count(UserID, day_nr,) %>%  
  filter(n > 1) %>% 
  nrow()
#there are 207 days per person that have multiple rows.

```

Next, we identified the number duplicates by creating a temporary dataframe specifically for them. To gain further insight into the differences per row, we followed the steps from the tutorial using `n_identical`. This showed that there are indeed multiple rows per person per day, for example two rows for person 106 on day 11.

```{r}
#making a seperate datatibble with only the dupliactes.
duplicates <- DayData_tidy %>% 
  count(UserID, day_nr) %>% 
  filter(n > 1)

print(duplicates)

#this makes sense, because in the dataframe 'duplicaties' there are as well 207 rows.

#doing the same but then how it's done in the tutorial.
DayData_tidy |>
  group_by(UserID, day_nr) |> 
  mutate(n_identical = n()) |> 
  filter(n_identical > 1) |>
  arrange( UserID, day_nr)|> 
  ungroup() |>
  slice(1:2)

#here we found out there are multiple rows per person per day, and the reason why differs (some drunk more over time or some stopped to early).

```

The data showed that the rows were duplicated for day and person, because some respondents did not finish the questionnaire on their first attempt and later filled it in again.

To correct this, the incomplete questionnaires were excluded. Another check was carried out to be sure all unfinished rows were deleted, and since all of these rows were completed, this step proved to be effective. After removing the unfinished entries, there were still 133 rows for the same day per person.

```{r}

#we can not use the persons who haven't finished the questionnaire. So we delete them from the dataset.
DayData_tidy <- DayData_tidy %>%
  filter(Finished == 1)

#recheck how many multiple rows per person per day are in the dataset.
DayData_tidy|> 
  count(UserID, day_nr,) %>%  
  filter(n > 1) %>% 
  nrow()
#there are 133 days per person that have multiple rows. This means they have multiple rows per person per day. 

#check example for differences per row.
DayData_tidy |>
  group_by(UserID, day_nr) |>
  mutate(n_identical = n()) |> 
  filter(n_identical > 1) |>
  arrange( UserID, day_nr)|> 
  ungroup() |>
  slice(1:2)
#this works and they all finished. The differences here are in the other variables (columns).

```

Next, we examined how the remaining duplicate rows differed from one another. It turned out that some respondents completed the questionnaire multiple times in a single day, but at different times. For example, four hours after the first entry, they might have reported more alcohol consumption or additional snacks. To address this, we chose to keep the most recent row and remove the earlier one(s).

```{r}

#now there are still some multiple rows per person per day. We'll pick only the most recent ones.
DayData_tidy <- DayData_tidy %>%
  mutate(datum = as.Date(EndDate)) %>%   
  group_by(UserID, datum) %>%             
  arrange(desc(EndDate)) %>%               
  slice(1) %>%                         
  ungroup()                  

#I accidentally wrote date in dutch. So here i redo it.
DayData_tidy <- DayData_tidy %>%
  rename(Date = datum)

```

In this step, the invalid values, for example the values containing text, were also and deleted.

```{r}

#some values have text in it. check it out.
  unique(DayData_tidy$Snack_Freq)
#around 17 of these answers. As this question does not appear in our group's individual research questions, we have decided to leave it as it is for now. 
  
#but first, some people typed something in the values (e.g. 3 glazen, of 3 uur). First we have to make these to characters. 
DayData_tidy <- DayData_tidy %>%  
  mutate(
    Alc_Occ = as.character(Alc_Occ),
    Alc_Freq = as.character(Alc_Freq),
    Alc_Soc = as.character(Alc_Soc),
    Sport_Occ = as.character(Sport_Occ),
    Sport_Freq = as.character(Sport_Freq),
    Sport_Soc = as.character(Sport_Soc),
    Snack_Occ = as.character(Snack_Occ),
    Snack_Freq = as.character(Snack_Freq),
    Snack_Soc = as.character(Snack_Soc)
)
                  
#change those so that only the numbers will be used. 
DayData_tidy <- DayData_tidy %>% 
  mutate(across(Alc_Occ:Snack_Freq, ~ ifelse(grepl("[0-9]", .), ., NA))) %>% #the two values with only text are now NA.
  mutate(across(Alc_Occ:Snack_Freq, parse_number))
  # the values with no number in it are now NAs.
  #parse_number already makes these values from characters to numbers again.
                  
```

Next, we checked again how many duplicate rows remained based on `UserID` and `day_nr`.

```{r}

#check how many multiple rows per person per day are in the dataset.
DayData_tidy|> 
  count(UserID, day_nr) %>%  
  filter(n > 1) %>% 
  nrow()
#now there are still 44 doubles.

#checking out how the rows differ from each other.
DayData_tidy |>
  group_by(UserID, day_nr) |>
  mutate(n_identical = n()) |> 
  filter(n_identical > 1) |>
  arrange( UserID, day_nr)|> 
  ungroup() |>
  slice(1:6)
#now we see that some people also have filled in the same day per person on different days. 

```

After keeping the most recent row per person per day, there were still 44 duplicate rows left. We examined how these rows differed from one another. It turned out that some respondents had entered data for the same day, but had actually completed the questionnaire on different dates. To ensure that the most recent entry was used in these cases as well, we relied not on the newly created `date` variable but on `EndDate`.

```{r}

#to fix that, we'll not use the new variable date to filter the most recent one, but the enddate.
DayData_tidy <- DayData_tidy %>%
  mutate(EndDate = as.POSIXct(EndDate)) %>%
  group_by(UserID, day_nr) %>%
  arrange(desc(EndDate)) %>%
  slice(1) %>%
  ungroup()

#check again how many multiple rows per person per day are in the dataset.
DayData_tidy %>% 
  count(UserID, day_nr) %>%  
  filter(n > 1) %>% 
  nrow()
#the result is 0. The primary key is UserID  x day_nr.

```

In the final check, it turned out that there are 0 rows for `UserID` x `day_nr`, so we can assume that the primary key of the Day Data set is `UserID` × `day_nr`. As a final step, a filter was used to check whether the `UserID` and `day_nr` contained missing values. This was not the case for `day_nr`, but it was for `UserID`. There were 30 such cases.

```{r}

#check for missing values for day_nr
DayData_tidy |> 
  filter(is.na(day_nr)) |> 
  print(n = Inf)

#check for missing values for UserID
DayData_tidy |> 
  filter(is.na(UserID)) |> 
  print(n = Inf)
```

```{r}

#removing the rows with NA in the UserID.
DayData_tidy <- DayData_tidy |> 
  filter(!is.na(UserID))

#recheck for missing values for UserID.
DayData_tidy |> 
  filter(is.na(UserID)) |> 
  print(n = Inf)
#this results in 0 rows, so mission completed.

```

### WeekData.csv

To determine the primary key of the week dataset, roughly the same steps were followed as for the day data. First, we looked at how many people were in the dataset, which was 270. Then we checked how many days were included in the dataset, which was 6, and also how many weeks were included, which was 6 as well. The day numbers were the same for each week: day 8, day 15, day 22, day 29, day 36, and day 43. Since these days are the same for each week number (for example, every week 1 has day number 8), `day_nr` is by reasoning not part of the primary key.

```{r}

WeekData_tidy <- WeekData

#check how many persons are included in the weekdataset.
WeekData_tidy |> 
  count(UserID) |> 
  nrow()
# there are 270 rows, so 270 unique persons. 

#check how many days are included in the weekdataset.
WeekData_tidy |> 
  count(day_nr) |> 
  nrow()
#there are 6 rows, so 6 days.

#check how many weeks are included in the weekdataset.
WeekData_tidy |> 
  count(week_nr) |> 
  nrow()
#there are 6 rows, so 6 weeks.

#we want to have an overview of the days and frequency.
WeekData_tidy |> 
  count(day_nr)
# this shows there are 6 days (8, 15, 22, 29, 36, 43), and 200+ rows per day.

#we want to have an overview of the weeks and frequency.
WeekData_tidy |> 
  count(week_nr)
# this shows there are 6 weeks (1,2,3,4,5,6), and 200+ rows per week

#this means the values in the day_nr are the same for every week. 

```

Next, we looked at the number of rows that would remain after counting by `UserID` and `week_nr`. A closer look at the duplicates per person and week revealed, similar to the daily dataset, that several respondents had completed the questionnaire more than once for the same week. The differences were also comparable: some entries were initially incomplete and later filled in, or values had changed between submissions. Therefore, we again decided to retain only the completed and most recent rows, excluding the others.

```{r}

#to check whether the primary key is week_nr x UserID (since the day_nr is the same for all week_nr per person)
WeekData_tidy|> 
  count(UserID, week_nr) |>  
  filter(n > 1) |> 
  nrow()
#results in 26 rows. 

#to see the duplicates as done in the tutorial.
WeekData_tidy |>
  group_by(UserID, week_nr) |>
  mutate(n_identical = n()) |> 
  filter(n_identical > 1) |>
  arrange( UserID, week_nr)|> 
  ungroup() |>
  slice(1:6)
#now we see that some people also have filled in the same week per person on different days. 

#we can not use the persons who haven't finished the questionnaire. So we delete them from the dataset.
WeekData_tidy <- WeekData_tidy |>
  filter(Finished == 1)

#to keep only the most recent rows. 
WeekData_tidy <- WeekData_tidy |>
  mutate(EndDate = as.POSIXct(EndDate)) |>
  group_by(UserID, week_nr) |>
  arrange(desc(EndDate)) |>
  slice(1) |>
  ungroup()

#check again the number of multiple rows per week per person
WeekData_tidy|> 
  count(UserID, week_nr) |> 
  filter(n > 1) |> 
  nrow()
#results in 0 rows. 
```

After this was carried out, the check for the primary key returned 0 rows, which means we can conclude that the primary key for the Week Data set is `UserID` x `week_nr`. Next, we checked whether there were any missing values in the primary keys. For the week data, there were no missing values in the `UserID` and `week_nr`.

```{r}

#checking for missing values in primary key. 
WeekData_tidy |> 
  filter(is.na(UserID))
#this results in 0 rows.

#checking for missing values in primary key. 
WeekData_tidy |> 
  filter(is.na(week_nr))
#this results in 0 rows.

```

## Principle 1

### DayData.csv

To investigate whether principle 1 was violated in the `Day Dataset`, it is necessary to explore the dataset. This dataset shows the occurrence, frequency, and sociality of alcohol use, sports, and snacking for each person per day. It also indicates how the person felt on that day. The primary key of the dataset is `UserID` x `day_nr`, as shown in the code before. All variables in this `Day Dataset` describe characteristics of the person per day. Therefore, the `Day Dataset` does not violate principle 1. It is also not necessary to split the `Day Dataset` into separate tibbles.

### WeekData.csv

Like the `Day Data`, the `Week Data` contains information per person per day, but aggregated by week. Each week, respondents completed the questionnaire on the same day, so the day_nr values are identical for each weekly entry. The primary key is `UserID` x `week_nr`.

The variables in this dataset cover the assessment of alcohol through 15 questions, as well as perceptions of the intention to consume alcohol in the upcoming week. Additionally, it includes how often the SNS app was used in the past week and how attentively it was viewed. All these variables/columns therefore refer to the person per week. It is therefore not necessary to split the dataset into multiple tibbles, as principle 1 is not violated.

## Principle 2

### DayData.csv

For principle 2, that each observation must have its own row, `Day Data` is not in violation. The repeated information, for example the values in `day_nr`, is necessary and dependent on the `UserID`. There are no columns that are actually values.

::: callout-caution
## This is a brief exploratory analysis that was ultimately not used, but which does show how we reasoned.
:::

However, it was briefly explored whether the 9 columns (`Alc_Occ, Alc_Freq, Alc_Soc, Sport_Occ, Sport_Freq, Sport_Soc, Snack_Occ, Snack_Freq, and Snack_Soc`) could be tidied up, since `Occ`, `Freq`, and `Soc` all measure the same concept, only varying by activity. Therefore, the column names were moved into rows using `pivot_longer` as first step. Because some values in these columns contain text, for example, respondents sometimes added words to their answers (e.g., “3 glasses”), it was first necessary to try converting them into character type. For this exploration, a temporary new dataset was created, namely `PersonDayActivity`.

```{r}
#some people typed something in the values (e.g. 3 glazen, of 3 uur). First, we have to make these to characters again. 
DayData_tidy <- DayData_tidy %>% 
  mutate(
    Alc_Occ = as.character (Alc_Occ),
    Alc_Freq = as.character (Alc_Freq),
    Alc_Soc = as.character (Alc_Soc),
    Sport_Occ = as.character (Sport_Occ),
    Sport_Freq = as.character (Sport_Freq),
    Sport_Soc = as.character (Sport_Soc),
    Snack_Occ = as.character (Snack_Occ),
    Snack_Freq = as.character (Snack_Freq),
    Snack_Soc = as.character (Snack_Soc)
    )
                  
#now they are al characters, we make the columns variables into values of the variable "Activity". 
PersonDayActivity <- DayData_tidy %>% 
  pivot_longer(
    cols = c(Alc_Occ, Alc_Freq, Alc_Soc,
    Sport_Occ, Sport_Freq, Sport_Soc,
    Snack_Occ, Snack_Freq, Snack_Soc), 
      names_to = 'Activity', 
      values_to = 'Value')
                 
```

At this point, the column names were in the rows, as you would do when addressing principle 2. However, it was still necessary to separate the activity from the `Occ`, `Freq`, and `Soc` values. This was done using the fuction `pivot_longer_delim`.

```{r}
#seperating the activity from the measurement.
PersonDayActivity <- PersonDayActivity %>% 
  separate_wider_delim(
  cols = Activity,
  delim = "_",
  names = c("Activity", "Measure"))
```

After that, the `Occ`, `Freq`, and `Soc` values were returned to separate columns.

```{r}
 
PersonDayActivity <- PersonDayActivity %>% 
  pivot_wider(
    names_from = Measure, values_from = Value)

```

This resulted in the following PersonDayAcitcity table. Note that an NA always appears in the `Freq` and `Soc` columns when the respondent selected 1 for `Occ` (1 means no alcohol, did not exercise, or did not snack).

```{r echo = FALSE}
library(DT)

htmltools::div(
  style = "zoom: 0.8;",
  datatable(PersonDayActivity |> 
    select(UserID, day_nr, Activity, Occ, Freq, Soc)))
```

The problem is that the `Activity` column at this moment contains values that actually represent separate variables or columns. Because of this, this exploration remains only an exploratory step, and we will not use this approach of trying to turn the rows into separate observations in the tidied dataset. However, it does show how a potential violation of principle 2 could have been resolved, although this was not actually the case.

Long story short, there is therefore no violation of principle 2. Subsequently, the column values were converted back to numbers.

```{r}

#remake these values from characters to numbers again. 
DayData_tidy <- DayData_tidy %>%
  mutate(
    Alc_Occ = as.numeric(Alc_Occ),
    Alc_Freq = as.numeric(Alc_Freq),
    Alc_Soc = as.numeric(Alc_Soc),
    Sport_Occ = as.numeric(Sport_Occ),
    Sport_Freq = as.numeric(Sport_Freq),
    Sport_Soc = as.numeric(Sport_Soc),
    Snack_Occ = as.numeric(Snack_Occ),
    Snack_Freq = as.numeric(Snack_Freq),
    Snack_Soc = as.numeric(Snack_Soc)
  )

```

### WeekData.csv

As in the Day Data, there are no violations of principle 2 in the Week Data. Each observation has its own row. The repeated information, such as the values in `week_nr`, is necessary and dependent on the `UserID`. There are no columns that are actually values.

## Principle 3

### DayData.csv

Day Data does not violate Principle 3. Each variable has its own column. The values in the rows do not represent information that should be separate variables themselves. Different types of information are not stored in the same column. Rather, related information is distributed across separate columns. For example, `Alc_Occ`, `Alc_Freq`, and `Alc_Soc` are distinct variables, each with its own column.

### WeekData.csv

Week Data does not violate Principle 3. Every variable is assigned to its own column and the row values do not contain information that should be treated as separate variables. Columns do not mix different types of information, since the related data is organized across multiple columns. For instance, `Alc_Ass_1` through `Alc_Ass_15` are individual variables, each represented by its own column.

It could, however, still possible to combine the 15 items for `Alc_Ass` by calculating their average using the mutate function. The same applies to `Alc_IN` and its three items.

```{r}
#making new variable with means of acl_ass1-15.
WeekData_tidy <- WeekData_tidy %>% 
  mutate(Alc_ass_mean = rowMeans(across(Alc_Ass1:Alc_Ass15), na.rm = TRUE))

#rounding in 2 decimals
WeekData_tidy <- WeekData_tidy %>%
  mutate(Alc_ass_mean = round(Alc_ass_mean, 2))

#making new variable with means of alc_in1-3.
WeekData_tidy <- WeekData_tidy %>% 
  mutate(Alc_in_mean = rowMeans(across(Alc_IN1:Alc_IN3), na.rm = TRUE))

#rounding in 2 decimals.
WeekData_tidy <- WeekData_tidy %>%
  mutate(Alc_in_mean = round(Alc_in_mean, 2))
```

## Principle 4

### DayData.csv

In the Day Data, principle 4 is not violated. Since the questions about frequency (`Alc_Freq`, `Sport_Freq`, and `Snack_Freq`) are open-ended, some respondents in the original raw data added words to their answers, for example “3 glazen” when indicating how much they had drunk that day. However, this issue was resolved once the data was cleaned during finding out the primary key. This was verified using the following code, which showed no irregular entries containing values.

```{r}
#finding out if there are still some values with text - Alc_Freq.
DayData_tidy %>%
  count(Alc_Freq) %>% 
  arrange(desc(Alc_Freq)) %>% 
  print(n = Inf)
#no other values than numbers in Alc_Freq.

#finding out if there are still some values with text - Sport_Freq.
DayData_tidy %>%
  count(Sport_Freq) %>% 
  arrange(desc(Sport_Freq)) %>% 
  print(n = Inf)
#no other values than numbers in Sport_Freq.

#finding out if there are still some values with text - Snack_Freq.
DayData_tidy %>%
  count(Snack_Freq) %>% 
  arrange(desc(Snack_Freq)) %>% 
  print(n = Inf)
#no other values than numbers in Snack_Freq.

```

To conclude, this will be the Day Data dataset after tidying:

```{r echo = FALSE}

htmltools::div(
  style = "zoom: 0.8;",
  datatable(DayData_tidy))

```

### WeekData.csv

In the Week Data, it was not necessary to split the values because they all consist of numbers, with no unnecessary text or values that needed separating. Each value has its own cell, and each cell contains only a single piece of information. Principle 4 was not violated.

To conclude, this will be the Week Stat dataset after tidying:

```{r echo = FALSE}

htmltools::div(
  style = "zoom: 0.8;",
  datatable(WeekData_tidy))
```

## Table Joining

## Description
