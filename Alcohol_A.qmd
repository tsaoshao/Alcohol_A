---
title: "Alcohol Data Tidying Report"
format: html
editor: visual
---

# 1 Introduction

In this report, we are going to present the process and rationale of our data tidying process of the data set `AlcoholData`, in terms of how the four data tidy principles were applied, how the relations and primary keys of each tibble were found and checked, and the way we joined the tibbles. All data tidying and report writing were done in R studio version 2025.09.2+418.

# 2 Data Tidying

## 2.1 Data Importing and Cleaning

All eight data sets were imported with `read_csv` under `tidyverse` and made use of. Each of them was renamed to lowercase with underscores for code clarity.

```{r}
#| label: data-importing

# tidyverse for data wrangling; lubridate for time parsing; janitor for converting variable names
library(tidyverse)
library(lubridate)
library(janitor)

# Import and rename data sets for clarity
comment <- read_csv("AlcoholData/Comment.csv")
day_data <- read_csv("AlcoholData/DayData.csv")
like <- read_csv("AlcoholData/Like.csv")
login <- read_csv("AlcoholData/Login.csv")
post <- read_csv("AlcoholData/Post.csv")
pre_survey <- read_csv("AlcoholData/Presurvey.csv")
user <- read_csv("AlcoholData/User.csv")
week_data <- read_csv("AlcoholData/WeekData.csv")
```

To make way for further analysis, variable names were transformed into lowercase form with underscores. `post_id` and `viewer_id` were converted to character because they are only categorical variables without numerical meaning. `post_comments`, `post_likes` and `post_views` were then converted from `double` to `integer` to avoid analytic errors. As time-sensitive variables like `post_time` and `sync_time` were recognised as datetime type by `read_csv()`, time parsing functions were not used.

```{r}
#| label: data-cleaning

comment <- comment |> 
  clean_names() |> 
  mutate(
    post_id = as.character(post_id),
    commenter_id = as.character(commenter_id)
  )

day_data <- day_data |> 
  clean_names() |> 
  mutate()

like <- like |> 
  clean_names() |> 
  mutate(
    post_id = as.character(post_id),
    liker_id = as.character(liker_id)
  )

login <- login |> 
  clean_names() |> 
  mutate(user_id = as.character(user_id))

post <- post |>
  clean_names()|>
  mutate(
    post_id = as.character(post_id),
    viewer_id = as.character(viewer_id),
    post_comments = as.integer(post_comments),
    post_likes = as.integer(post_likes),
    post_views = as.integer(post_views)
  )
```

## 2.2 Principle 1: Each type of case must have its own tibble

To decide if principle 1 was violated in each data set, we used `group_by()`, `summarise()` and `count()` to identify type of cases in each raw data set.

### 2.2.1 Comment.csv

For this file

```{r}
comment |> 
  group_by(post_id, commenter_id) |> 
  summarise(n_distinct = n_distinct(comment_time), .groups = "drop") |> 
  count(n_distinct)
```

### 2.2.2 DayData.csv

### 2.2.3 Like.csv

```{r}
like |> 
  distinct(post_id, liker_id, .keep_all = TRUE)
```

### 2.2.4 Login.csv

```{r}

```

### 2.2.5 Post.csv

Principle 1 was applied to split post attributes and viewer attributes into two different tibbles.

```{r}
# FIXME: Renaming variables by removing uppercase and adding underscores.
# Convert post ID and viewer ID to character to avoid accidental numeric behaviour.
# Convert from double to integer type.
# Convert from double to datetime type.

```

We checked for the possible type of cases in `post.csv`, after a few trials, we found `viewer_id` does not belong to the type of case of "post".

```{r}
# For each value of the selected type of case, calculate the number of distinct values of the selected variable.
# Count number of distinct variable values per type of case: 
# If 1, then variable has unique value for each case.
post |> 
  group_by(post_id) |>
  summarise(n_distinct = n_distinct(viewer_id), .groups = "drop") |>
  count(n_distinct)
```

The raw data set `Post.csv` was then split into two separate tibbles based on the type of cases. `post_sub_1` included information about each post, `post_sub_2` contained only two variables: `post_id` and `viwer_id`. `distinct()` was used on the primary key(s) to eliminate repetitions, as well as further clarity. `post_sub_1` was thus reduced to a parsimonious data set of 547 rows from 91031 rows.

```{r}
# Splitting post and view information into 2 separate tibbles
# Transform tidied post_sub_1 to make it clearer
post_sub_1 <- post |> 
  select(post_id : sync_time) |> 
  distinct(post_id, .keep_all = TRUE) 

# FIXME: Ensure unique (post_id, viewer_id) rows
post_sub_2 <- post |> 
  select(post_id, viewer_id) |> 
  distinct(post_id, viewer_id, .keep_all = TRUE)
```

### 2.2.6 Presurvey.csv

This file...

### 2.2.7 User.csv

### 2.2.8 WeekData.csv

## 2.3 Principle 2: Each observation must have its own row

## 2.4 Principle 3: Each variable must have its own column

## 2.5 Principle 4: Each value must have its own cell

# 3 Table relations and primary key checking

## 3.1 Comment.csv

```{r}
comment |> 
  count(post_id, comment_time, commenter_id) |> 
  filter(n > 1)
```

## 3.3 Like.csv

```{r}
like |> 
  count(post_id, liker_id) |> 
  filter(n > 1)
```

## 3.4 Login.csv

```{r}
login |> 
  count(user_id, user_login_time) |> 
  filter(n > 1)
```

## 3.5 Post.csv

### 3.5.1 post_sub_1

```{r}
post_sub_1 |> 
  count(post_id) |> 
  filter(n > 1)
```

### 3.5.2 post_sub_2

```{r}
post_sub_2 |> 
  count(post_id, viewer_id) |> 
  filter(n > 1)
```

# 4 Table Joining

# 5 Description
