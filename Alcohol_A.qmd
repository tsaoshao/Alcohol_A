---
title: "Alcohol Data Tidying Report"
format: html
editor: visual
---

# 1 Introduction

In this report, we are going to present the process and rationale of our data tidying process of the data set `AlcoholData`, in terms of how the four data tidy principles were applied, how the relations and primary keys of each tibble were found and checked, and the way we joined the tidied tibbles. All data tidying and report writing were done in R studio version 2025.09.2+418.

# 2 Data Tidying

## 2.1 Data Importing and Cleaning

All eight data sets were imported with `read_csv` under `tidyverse` and made use of. Each of them was renamed to lowercase with underscores for code clarity.

```{r}
#| label: data-importing

# tidyverse for data wrangling; lubridate for time parsing; janitor for converting variable names
library(tidyverse)
library(lubridate)
library(janitor)

# Import and rename data sets for clarity
comment <- read_csv("AlcoholData/Comment.csv")
day_data <- read_csv("AlcoholData/DayData.csv")
like <- read_csv("AlcoholData/Like.csv")
login <- read_csv("AlcoholData/Login.csv")
post <- read_csv("AlcoholData/Post.csv")
pre_survey <- read_csv("AlcoholData/Presurvey.csv")
user <- read_csv("AlcoholData/User.csv")
week_data <- read_csv("AlcoholData/WeekData.csv")
```

To make way for further analysis, variable names were transformed into lowercase form with underscores. `post_id` and `viewer_id` were converted to character because they are only categorical variables without numerical meaning. `post_comments`, `post_likes` and `post_views` were then converted from `double` to `integer` to avoid analytic errors. As time-sensitive variables like `post_time` and `sync_time` were recognised as datetime type by `read_csv()`, time parsing functions were not used.

```{r}
#| label: data-cleaning

comment <- comment |> 
  clean_names() |> 
  mutate(
    post_id = as.character(post_id),
    commenter_id = as.character(commenter_id)
  )

day_data <- day_data |> 
  clean_names() |> 
  mutate(
    user_id = as.character(user_id)
    # Add more type conversions here as needed, e.g.:
    # day_num = as.integer(day_num),
    # total_drinks = as.numeric(total_drinks),
  )

like <- like |> 
  clean_names() |> 
  mutate(
    post_id = as.character(post_id),
    liker_id = as.character(liker_id)
  )

login <- login |> 
  clean_names() |> 
  mutate(user_id = as.character(user_id))

# Convert post ID and viewer ID to character to avoid accidental numeric behaviour.
# Convert from double to integer type.
# Convert from double to datetime type.
post <- post |>
  clean_names()|>
  mutate(
    post_id = as.character(post_id),
    viewer_id = as.character(viewer_id),
    post_comments = as.integer(post_comments),
    post_likes = as.integer(post_likes),
    post_views = as.integer(post_views)
  )

pre_survey <- pre_survey |>
  clean_names() |>
  mutate(
    user_id = as.character(user_id)
    # TODO: Add additional type conversions for survey variables as needed
  )

user <- user |>
  clean_names() |>
  mutate(
    user_id = as.character(user_id)
    # TODO: Add further type conversions, e.g. age = as.integer(age), gender = as.character(gender), etc.
  )

week_data <- week_data |>
  clean_names() |>
  mutate(
    user_id = as.character(user_id)
    # TODO: Add more, e.g. week_num = as.integer(week_num), total_drinks = as.numeric(total_drinks), etc.
  )
```

## 2.2 Principle 1: Each type of case must have its own tibble

To decide if principle 1 was violated in each data set, we used `group_by()`, `summarise()` and `count()` to identify type of cases in each raw data set.

### 2.2.1 Comment.csv

The type of case in this data set is an individual comment made by a user on a post at a certain time, defined by (`post_id`, `commenter_id`, `comment_time`). We counted (`post_id`, `commenter_id`, `comment_time`) to check for duplicate rows, which would not violate Principle 1, but multiple observations for a case would influence the clarity.

```{r}
# Type of case check
comment |> 
  group_by(post_id, comment_time, commenter_id) |> 
  summarise(n_distinct = n_distinct(post_id, comment_time, commenter_id, comment_content), .groups = "drop") |> 
  count(n_distinct)

# Check for duplicates
comment_dupes <- comment |>
  count(post_id, commenter_id, comment_time) |>
  filter(n > 1)

# View duplicate comments
comment |> semi_join(comment_dupes, by = c("post_id", "commenter_id", "comment_time"))
```

Duplicate rows were found, they should be removed so each row represents a unique comment event.

```{r}
# Use .keep_all = TRUE so that, for each group of duplicated identifiers, we retain all columns from the first occurrence and preserve other information in the tibble.
comment_tidied <- comment |>
  distinct(post_id, commenter_id, comment_time, .keep_all = TRUE)
```

Each row is now an individual comment action (by a commenter, on a post, at a time).

### 2.2.2 DayData.csv

### 2.2.3 Like.csv

The type of case in `Like.csv` is a unique like event, defined by (`post_id`, `liker_id`). It does not violate principe 1 as well, we check for duplicate (`post_id`, `liker_id`) pairs to ensure data set clarity.

```{r}
# Type of case check
like |> 
  group_by(post_id, liker_id) |> 
  summarise(n_distinct = n_distinct(post_id, liker_id), .groups = "drop") |> 
  count(n_distinct)

# Check for duplicates
like_dupes <- like |>
  count(post_id, liker_id) |>
  filter(n > 1)

# View duplicate likers
like |> semi_join(like_dupes, by = c("post_id", "liker_id"))
```

We removed any duplicate rows so each row corresponds to a unique like event.

```{r}
# .keep_all ensures other information is preserved.
like_tidied <- like |>
  distinct(post_id, liker_id, .keep_all = TRUE)
```

Each row now is a unique user liking a post.

### 2.2.4 Login.csv

In this data set, the case type is a login event, defined by (`user_id`, `user_login_time`). Priciples 1 is not violated here, but We checked for duplicate login events per user to prevent further analytic errors.

```{r}
# Type of case check
login |> 
  group_by(user_id, user_login_time) |> 
  summarise(n_distinct = n_distinct(user_id, user_login_time), .groups = "drop") |> 
  count(n_distinct)

# Check for duplicates
login_dupes <- login |>
  count(user_id, user_login_time) |>
  filter(n > 1)
```

No duplicated rows were found. Each row is a single login event,representing a login by a user at a specific time.

### 2.2.5 Post.csv

Principle 1 was applied to split post attributes and viewer attributes into two different tibbles.


We checked for the possible type of cases in `Post.csv`.`viewer_id` was found not belong to the type of case of "post".

```{r}
# Check for violation (multiple views per post)
post |> 
  group_by(post_id) |>
  summarise(n_distinct = n_distinct(viewer_id), .groups = "drop") |>
  count(n_distinct)
```

The raw data set `Post.csv` was then split into two separate tibbles based on the type of cases. `post_sub1` included information about each post, `post_sub2` contained only two variables: `post_id` and `viwer_id`. `distinct()` was used on the primary key(s) to remove duplicates. `post_sub1` was thus reduced to a parsimonious data set of 547 rows from 91031 rows.

```{r}
# Splitting post and view information into 2 separate tibbles
# Adjust to select post-only variables
# Each row: one post
post_sub1 <- post |> 
  select(post_id : sync_time) |> 
  distinct(post_id, .keep_all = TRUE) 

# View events
# Each row: one unique (post, viewer) event
post_sub2 <- post |> 
  select(post_id, viewer_id) |> 
  distinct(post_id, viewer_id, .keep_all = TRUE)
```

Now for `post_sub1`, each row is a unique post; for `post_sub2`, each row is a unique view event (a user viewing a post).

### 2.2.6 Presurvey.csv

This file...

### 2.2.7 User.csv

### 2.2.8 WeekData.csv

## 2.3 Principle 2: Each observation must have its own row

## 2.4 Principle 3: Each variable must have its own column

## 2.5 Principle 4: Each value must have its own cell

# 3 Table relations and primary key checking

## 3.1 Comment.csv

```{r}
comment_tidied |> 
  count(post_id, comment_time, commenter_id) |> 
  filter(n > 1)
```

## 3.3 Like.csv

```{r}
like_tidied |> 
  count(post_id, liker_id) |> 
  filter(n > 1)
```

## 3.4 Login.csv

```{r}
login |> 
  count(user_id, user_login_time) |> 
  filter(n > 1)
```

## 3.5 Post.csv

### 3.5.1 post_sub_1

```{r}
post_sub1 |> 
  count(post_id) |> 
  filter(n > 1)
```

### 3.5.2 post_sub_2

```{r}
post_sub2 |> 
  count(post_id, viewer_id) |> 
  filter(n > 1)
```

# 4 Table Joining

# 5 Description
